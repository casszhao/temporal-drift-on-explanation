2022-05-20 18:42:56 INFO     Running on cuda : True
2022-05-20 18:42:56 INFO     config  : 
 ----------------------
2022-05-20 18:42:56 INFO     dataset : yelp_full
2022-05-20 18:42:56 INFO     data_dir : /jmain02/home/J2AD003/txk58/zxz22-txk58/extract_rationales/extract_rationales/datasets/yelp_full/data/
2022-05-20 18:42:56 INFO     model_dir : /jmain02/home/J2AD003/txk58/zxz22-txk58/extract_rationales/extract_rationales/ft_models/yelp_full/
2022-05-20 18:42:56 INFO     seed : 412
2022-05-20 18:42:56 INFO     evaluate_models : False
2022-05-20 18:42:56 INFO     inherently_faithful : None
2022-05-20 18:42:56 INFO     use_tasc : False
2022-05-20 18:42:56 INFO     importance_metric : None
2022-05-20 18:42:56 INFO     rationale_length : 0.1
2022-05-20 18:42:56 INFO     batch_size : 8
2022-05-20 18:42:56 INFO     lr_bert : 1e-05
2022-05-20 18:42:56 INFO     lr_classifier : 0.0001
2022-05-20 18:42:56 INFO     model : bert-base-uncased
2022-05-20 18:42:56 INFO     ood_dataset_1 : yelp_ood1
2022-05-20 18:42:56 INFO     ood_dataset_2 : yelp_ood2
2022-05-20 18:42:56 INFO     epochs : 5
2022-05-20 18:42:56 INFO     model_abbreviation : bert
2022-05-20 18:42:56 INFO     evaluation_dir : None
2022-05-20 18:42:56 INFO     extracted_rationale_dir : None
2022-05-20 18:42:56 INFO     query : False
2022-05-20 18:42:56 INFO     stage_of_proj : train
2022-05-20 18:42:56 INFO     ood_rat_1 : 0.1
2022-05-20 18:42:56 INFO     ood_rat_2 : 0.1
2022-05-20 18:42:56 INFO     embed_model : None
2022-05-20 18:42:56 INFO     
 ----------------------
2022-05-20 18:44:03 INFO     2022-05-20_18:42
2022-05-20 18:44:03 INFO     Finetune BERT for: yelp_full
2022-05-20 18:44:03 INFO     Finetune BERT for: yelp_full
2022-05-20 18:44:41 INFO      \ ------------------  batch size: 8
2022-05-20 18:44:41 INFO      \ -------------------- learning rate: 0.0001
2022-05-20 18:45:11 INFO     ***************************************
2022-05-20 18:45:11 INFO     Training on seed 412
2022-05-20 18:45:11 INFO     *saving checkpoint every 498 iterations
2022-05-20 18:45:11 INFO     +++++++++++++ epochs:  5
2022-05-20 18:51:38 INFO     *** epoch - 1 | train loss - 11.61 | dev f1 - 0.118 | dev loss - 11.51
2022-05-20 18:58:01 INFO     *** epoch - 2 | train loss - 11.5 | dev f1 - 0.118 | dev loss - 11.42
2022-05-20 19:04:29 INFO     *** epoch - 3 | train loss - 11.48 | dev f1 - 0.118 | dev loss - 11.48
2022-05-20 19:10:50 INFO     *** epoch - 4 | train loss - 11.45 | dev f1 - 0.118 | dev loss - 11.44
2022-05-20 19:17:16 INFO     *** epoch - 5 | train loss - 11.43 | dev f1 - 0.118 | dev loss - 11.42
2022-05-20 19:17:18 INFO      \ -------------------- learning rate: 0.0005
2022-05-20 19:17:22 INFO     ***************************************
2022-05-20 19:17:22 INFO     Training on seed 412
2022-05-20 19:17:22 INFO     *saving checkpoint every 498 iterations
2022-05-20 19:17:22 INFO     +++++++++++++ epochs:  5
2022-05-20 19:23:43 INFO     *** epoch - 1 | train loss - 11.95 | dev f1 - 0.118 | dev loss - 11.47
2022-05-20 19:30:06 INFO     *** epoch - 2 | train loss - 11.51 | dev f1 - 0.118 | dev loss - 11.43
2022-05-20 19:36:32 INFO     *** epoch - 3 | train loss - 11.57 | dev f1 - 0.118 | dev loss - 11.89
2022-05-20 19:42:54 INFO     *** epoch - 4 | train loss - 11.54 | dev f1 - 0.118 | dev loss - 11.46
2022-05-20 19:49:21 INFO     *** epoch - 5 | train loss - 11.45 | dev f1 - 0.118 | dev loss - 11.42
2022-05-20 19:49:23 INFO      \ -------------------- learning rate: 1e-05
2022-05-20 19:49:27 INFO     ***************************************
2022-05-20 19:49:27 INFO     Training on seed 412
2022-05-20 19:49:27 INFO     *saving checkpoint every 498 iterations
2022-05-20 19:49:27 INFO     +++++++++++++ epochs:  5
2022-05-20 19:55:55 INFO     *** epoch - 1 | train loss - 7.18 | dev f1 - 0.584 | dev loss - 6.36
2022-05-20 20:02:17 INFO     *** epoch - 2 | train loss - 5.18 | dev f1 - 0.594 | dev loss - 6.96
2022-05-20 20:08:41 INFO     *** epoch - 3 | train loss - 3.96 | dev f1 - 0.567 | dev loss - 8.13
2022-05-20 20:15:05 INFO     *** epoch - 4 | train loss - 3.0 | dev f1 - 0.598 | dev loss - 9.33
2022-05-20 20:21:29 INFO     *** epoch - 5 | train loss - 2.3 | dev f1 - 0.596 | dev loss - 10.39
2022-05-20 20:21:31 INFO      \ -------------------- learning rate: 2e-05
2022-05-20 20:21:35 INFO     ***************************************
2022-05-20 20:21:35 INFO     Training on seed 412
2022-05-20 20:21:35 INFO     *saving checkpoint every 498 iterations
2022-05-20 20:21:35 INFO     +++++++++++++ epochs:  5
2022-05-20 20:28:00 INFO     *** epoch - 1 | train loss - 7.36 | dev f1 - 0.557 | dev loss - 6.47
2022-05-20 20:34:33 INFO     *** epoch - 2 | train loss - 5.12 | dev f1 - 0.577 | dev loss - 7.1
2022-05-20 20:40:57 INFO     *** epoch - 3 | train loss - 3.39 | dev f1 - 0.563 | dev loss - 8.98
2022-05-20 20:47:22 INFO     *** epoch - 4 | train loss - 2.03 | dev f1 - 0.583 | dev loss - 13.95
2022-05-20 20:53:46 INFO     *** epoch - 5 | train loss - 1.25 | dev f1 - 0.588 | dev loss - 16.83
2022-05-20 20:53:48 INFO      \ -------------------- learning rate: 4e-05
2022-05-20 20:53:52 INFO     ***************************************
2022-05-20 20:53:52 INFO     Training on seed 412
2022-05-20 20:53:52 INFO     *saving checkpoint every 498 iterations
2022-05-20 20:53:52 INFO     +++++++++++++ epochs:  5
2022-05-20 21:00:17 INFO     *** epoch - 1 | train loss - 7.69 | dev f1 - 0.55 | dev loss - 6.69
