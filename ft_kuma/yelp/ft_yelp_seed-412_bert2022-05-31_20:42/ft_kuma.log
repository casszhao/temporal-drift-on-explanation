2022-05-31 20:42:51 INFO     Running on cuda : True
2022-05-31 20:42:51 INFO     config  : 
 ----------------------
2022-05-31 20:42:51 INFO     dataset : yelp
2022-05-31 20:42:51 INFO     data_dir : /home/cass/PycharmProjects/ood_time/extract_rationales/datasets/yelp/data/
2022-05-31 20:42:51 INFO     model_dir : /home/cass/PycharmProjects/ood_time/extract_rationales/ft_kuma/yelp/
2022-05-31 20:42:51 INFO     seed : 412
2022-05-31 20:42:51 INFO     evaluate_models : False
2022-05-31 20:42:51 INFO     inherently_faithful : kuma
2022-05-31 20:42:51 INFO     use_tasc : False
2022-05-31 20:42:51 INFO     importance_metric : None
2022-05-31 20:42:51 INFO     rationale_length : 0.2
2022-05-31 20:42:51 INFO     batch_size : 64
2022-05-31 20:42:51 INFO     lr_bert : 1e-05
2022-05-31 20:42:51 INFO     lr_classifier : 0.0001
2022-05-31 20:42:51 INFO     model : bert-base-uncased
2022-05-31 20:42:51 INFO     ood_dataset_1 : yelp_ood1
2022-05-31 20:42:51 INFO     ood_dataset_2 : yelp_ood2
2022-05-31 20:42:51 INFO     epochs : 20
2022-05-31 20:42:51 INFO     model_abbreviation : kuma-bert
2022-05-31 20:42:51 INFO     evaluation_dir : None
2022-05-31 20:42:51 INFO     extracted_rationale_dir : None
2022-05-31 20:42:51 INFO     query : False
2022-05-31 20:42:51 INFO     stage_of_proj : train
2022-05-31 20:42:51 INFO     ood_rat_1 : 0.2
2022-05-31 20:42:51 INFO     ood_rat_2 : 0.2
2022-05-31 20:42:51 INFO     embed_model : glove.840B.300d
2022-05-31 20:42:51 INFO     
 ----------------------
2022-05-31 20:42:53 INFO     2022-05-31_20:42
2022-05-31 20:42:53 INFO     Finetune BERT for: yelp
2022-05-31 20:42:53 INFO     Finetune BERT for: yelp
2022-05-31 20:42:56 INFO     ***************************************
2022-05-31 20:42:56 INFO     Training on seed 412
2022-05-31 20:42:56 INFO     *saving checkpoint every 45 iterations
2022-05-31 20:42:56 INFO     +++++++++++++ epochs:  20
2022-05-31 20:43:11 INFO     *** epoch - 1 | train loss - 102.24 | dev f1 - 0.12 | dev loss - 100.91
2022-05-31 20:43:26 INFO     *** epoch - 2 | train loss - 100.4 | dev f1 - 0.113 | dev loss - 98.39
2022-05-31 20:43:42 INFO     *** epoch - 3 | train loss - 97.77 | dev f1 - 0.111 | dev loss - 94.99
2022-05-31 20:43:57 INFO     *** epoch - 4 | train loss - 94.41 | dev f1 - 0.111 | dev loss - 93.71
2022-05-31 20:44:13 INFO     *** epoch - 5 | train loss - 94.66 | dev f1 - 0.111 | dev loss - 95.92
2022-05-31 20:44:29 INFO     *** epoch - 6 | train loss - 98.79 | dev f1 - 0.111 | dev loss - 102.45
2022-05-31 20:44:44 INFO     *** epoch - 7 | train loss - 110.53 | dev f1 - 0.111 | dev loss - 120.1
2022-05-31 20:44:59 INFO     *** epoch - 8 | train loss - 141.13 | dev f1 - 0.111 | dev loss - 163.13
2022-05-31 20:45:14 INFO     *** epoch - 9 | train loss - 178.32 | dev f1 - 0.111 | dev loss - 121.44
2022-05-31 20:45:30 INFO     *** epoch - 10 | train loss - 74.53 | dev f1 - 0.111 | dev loss - 64.22
2022-05-31 20:45:47 INFO     *** epoch - 11 | train loss - 64.03 | dev f1 - 0.111 | dev loss - 64.71
2022-05-31 20:46:03 INFO     *** epoch - 12 | train loss - 67.51 | dev f1 - 0.111 | dev loss - 69.78
2022-05-31 20:46:19 INFO     *** epoch - 13 | train loss - 72.32 | dev f1 - 0.111 | dev loss - 74.34
2022-05-31 20:46:34 INFO     *** epoch - 14 | train loss - 76.37 | dev f1 - 0.111 | dev loss - 78.14
2022-05-31 20:46:49 INFO     *** epoch - 15 | train loss - 79.6 | dev f1 - 0.111 | dev loss - 80.93
2022-05-31 20:47:04 INFO     *** epoch - 16 | train loss - 82.18 | dev f1 - 0.111 | dev loss - 83.24
2022-05-31 20:47:19 INFO     *** epoch - 17 | train loss - 84.22 | dev f1 - 0.111 | dev loss - 85.11
2022-05-31 20:47:34 INFO     *** epoch - 18 | train loss - 85.82 | dev f1 - 0.111 | dev loss - 86.51
2022-05-31 20:47:48 INFO     *** epoch - 19 | train loss - 87.05 | dev f1 - 0.111 | dev loss - 87.63
2022-05-31 20:48:03 INFO     *** epoch - 20 | train loss - 88.05 | dev f1 - 0.111 | dev loss - 88.52
2022-05-31 20:48:04 INFO     
        yelp ----
2022-05-31 20:48:04 INFO     
        InDomain
        mean -> 0.0
        std ->  0.0
        all ->  0.0
    
