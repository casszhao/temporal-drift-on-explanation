2022-05-31 20:30:34 INFO     Running on cuda : True
2022-05-31 20:30:34 INFO     config  : 
 ----------------------
2022-05-31 20:30:34 INFO     dataset : yelp
2022-05-31 20:30:34 INFO     data_dir : /home/cass/PycharmProjects/ood_time/extract_rationales/datasets/yelp/data/
2022-05-31 20:30:34 INFO     model_dir : /home/cass/PycharmProjects/ood_time/extract_rationales/ft_kuma/yelp/
2022-05-31 20:30:34 INFO     seed : 412
2022-05-31 20:30:34 INFO     evaluate_models : False
2022-05-31 20:30:34 INFO     inherently_faithful : kuma
2022-05-31 20:30:34 INFO     use_tasc : False
2022-05-31 20:30:34 INFO     importance_metric : None
2022-05-31 20:30:34 INFO     rationale_length : 0.2
2022-05-31 20:30:34 INFO     batch_size : 64
2022-05-31 20:30:34 INFO     lr_bert : 1e-05
2022-05-31 20:30:34 INFO     lr_classifier : 0.0001
2022-05-31 20:30:34 INFO     model : bert-base-uncased
2022-05-31 20:30:34 INFO     ood_dataset_1 : yelp_ood1
2022-05-31 20:30:34 INFO     ood_dataset_2 : yelp_ood2
2022-05-31 20:30:34 INFO     epochs : 20
2022-05-31 20:30:34 INFO     model_abbreviation : kuma-bert
2022-05-31 20:30:34 INFO     evaluation_dir : None
2022-05-31 20:30:34 INFO     extracted_rationale_dir : None
2022-05-31 20:30:34 INFO     query : False
2022-05-31 20:30:34 INFO     stage_of_proj : train
2022-05-31 20:30:34 INFO     ood_rat_1 : 0.2
2022-05-31 20:30:34 INFO     ood_rat_2 : 0.2
2022-05-31 20:30:34 INFO     embed_model : glove.840B.300d
2022-05-31 20:30:34 INFO     
 ----------------------
2022-05-31 20:30:36 INFO     2022-05-31_20:30
2022-05-31 20:30:36 INFO     Finetune BERT for: yelp
2022-05-31 20:30:36 INFO     Finetune BERT for: yelp
2022-05-31 20:30:39 INFO     ***************************************
2022-05-31 20:30:39 INFO     Training on seed 412
2022-05-31 20:30:39 INFO     *saving checkpoint every 45 iterations
2022-05-31 20:30:39 INFO     +++++++++++++ epochs:  20
2022-05-31 20:30:54 INFO     *** epoch - 1 | train loss - 102.2 | dev f1 - 0.119 | dev loss - 100.88
2022-05-31 20:31:10 INFO     *** epoch - 2 | train loss - 100.32 | dev f1 - 0.113 | dev loss - 98.27
2022-05-31 20:31:25 INFO     *** epoch - 3 | train loss - 97.55 | dev f1 - 0.111 | dev loss - 94.68
2022-05-31 20:31:41 INFO     *** epoch - 4 | train loss - 93.86 | dev f1 - 0.111 | dev loss - 92.9
2022-05-31 20:31:57 INFO     *** epoch - 5 | train loss - 93.25 | dev f1 - 0.111 | dev loss - 93.82
2022-05-31 20:32:13 INFO     *** epoch - 6 | train loss - 95.12 | dev f1 - 0.111 | dev loss - 96.95
2022-05-31 20:32:28 INFO     *** epoch - 7 | train loss - 100.92 | dev f1 - 0.111 | dev loss - 105.74
2022-05-31 20:32:44 INFO     *** epoch - 8 | train loss - 116.45 | dev f1 - 0.111 | dev loss - 129.14
2022-05-31 20:32:59 INFO     *** epoch - 9 | train loss - 156.74 | dev f1 - 0.111 | dev loss - 183.47
2022-05-31 20:33:15 INFO     *** epoch - 10 | train loss - 182.48 | dev f1 - 0.111 | dev loss - 84.66
2022-05-31 20:33:31 INFO     *** epoch - 11 | train loss - 64.14 | dev f1 - 0.111 | dev loss - 59.33
2022-05-31 20:33:47 INFO     *** epoch - 12 | train loss - 60.1 | dev f1 - 0.111 | dev loss - 61.37
2022-05-31 20:34:02 INFO     *** epoch - 13 | train loss - 64.58 | dev f1 - 0.111 | dev loss - 67.22
2022-05-31 20:34:17 INFO     *** epoch - 14 | train loss - 70.08 | dev f1 - 0.111 | dev loss - 72.52
2022-05-31 20:34:32 INFO     *** epoch - 15 | train loss - 74.61 | dev f1 - 0.111 | dev loss - 76.44
2022-05-31 20:34:48 INFO     *** epoch - 16 | train loss - 78.23 | dev f1 - 0.111 | dev loss - 79.7
2022-05-31 20:35:03 INFO     *** epoch - 17 | train loss - 81.11 | dev f1 - 0.111 | dev loss - 82.33
2022-05-31 20:35:18 INFO     *** epoch - 18 | train loss - 83.37 | dev f1 - 0.111 | dev loss - 84.32
2022-05-31 20:35:34 INFO     *** epoch - 19 | train loss - 85.12 | dev f1 - 0.111 | dev loss - 85.9
2022-05-31 20:35:49 INFO     *** epoch - 20 | train loss - 86.53 | dev f1 - 0.111 | dev loss - 87.16
2022-05-31 20:35:50 INFO     
        yelp ----
